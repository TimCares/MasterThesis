{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rich.progress import track\n",
    "from bpe_encoder import get_bpe_encoder\n",
    "import PIL\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "import textwrap\n",
    "import os\n",
    "from datamodules import COCOCaptionsDataModule\n",
    "from models.SHRe import SHRePreTrainingLightningModule\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_args = {\n",
    "    \"data_path\": \"/workspace\",\n",
    "    \"num_max_bpe_tokens\": 64,\n",
    "    \"task\": \"captioning\",\n",
    "    \"color_jitter\": None,\n",
    "    \"beit_transforms\": False,\n",
    "    \"crop_scale\": [1.0, 1.0],\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 4,\n",
    "    \"shuffle\": False,\n",
    "    \"drop_last\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"\"\n",
    "model_path = os.path.join(\"/workspace/models\", version, 'fp32_last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_bpe_encoder('/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_retrieval(model, dataloader, device):\n",
    "    img_embeds = []\n",
    "    imgs = []\n",
    "    text_embeds = []\n",
    "    texts = []\n",
    "    coco_to_id = dict()\n",
    "\n",
    "    for i, batch in track(enumerate(dataloader)):\n",
    "        image = batch['image'].to(device)\n",
    "        text = batch['text'].to(device)\n",
    "        padding_mask = batch['padding_mask'].to(device) if 'padding_mask' in batch else None\n",
    "        # encoding also normalizes the output\n",
    "        img_emb = model.encode_image(image=image)['x']\n",
    "        text_emb = model.encode_text(text=text, padding_mask=padding_mask)['x']\n",
    "        img_embeds.append(img_emb)\n",
    "        text_embeds.append(text_emb)\n",
    "        imgs.append(batch['image'])\n",
    "        texts.append(batch['text'])\n",
    "        for i, img_id in enumerate(batch['id']):\n",
    "            if img_id.item() not in coco_to_id:\n",
    "                coco_to_id[img_id.item()] = [i]\n",
    "            else:\n",
    "                coco_to_id[img_id.item()].append(i)\n",
    "\n",
    "    img_embeds = torch.cat(img_embeds, dim=0)\n",
    "    text_embeds = torch.cat(text_embeds, dim=0)\n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    texts = torch.cat(texts, dim=0)\n",
    "    scores = img_embeds @ text_embeds.t()\n",
    "    return scores, coco_to_id, imgs, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_for_candidate(scores:torch.Tensor, idx_mapper, coco_id, from_modality, n=5):\n",
    "    dim = 0 if from_modality == 'image' else 1\n",
    "    idx = idx_mapper[coco_id]\n",
    "    return scores[idx].topk(n, dim=dim, largest=True, sorted=True).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(data, from_modality, n):\n",
    "    cols = n+1\n",
    "    rows = len(data)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(18, 15))\n",
    "\n",
    "    axes[0, 0].set_title(\"Query\")\n",
    "    for j in range(1, cols):\n",
    "        axes[0, j].set_title(f\"Retrieval #{j}\")\n",
    "\n",
    "    for i, key in enumerate(data.keys()):\n",
    "        q = data[key][0]\n",
    "\n",
    "        axes[i, 0].text(-0.2, 0.5, f\"COCO #{key}\", transform=axes[i, 0].transAxes, va='center', ha='right')\n",
    "\n",
    "        if from_modality == 'image':\n",
    "            axes[i, 0].imshow(q.permute(1, 2, 0))\n",
    "        else:\n",
    "            axes[i, 0].text(0.5, 0.5, q, fontsize=18, ha='center', va='center')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        for j in range(1, cols):\n",
    "            source = data[key][1][j-1]\n",
    "            if from_modality == 'image':\n",
    "                axes[i, j].text(0.5, 0.5, textwrap.fill(source, 15), fontsize=18, ha='center', va='center')\n",
    "            else:\n",
    "                axes[i, j].imshow(source.permute(1, 2, 0))\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    line_x_position = (axes[0, 0].get_position().x1 + axes[0, 1].get_position().x0) / 2\n",
    "\n",
    "    line = matplotlib.lines.Line2D([line_x_position, line_x_position], [0, 1], transform=fig.transFigure, color='black', linewidth=1)\n",
    "    fig.add_artist(line)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_retrievals(scores:torch.Tensor, idx_mapper, coco_ids, imgs, texts, from_modality, n=5):\n",
    "    result_dict = dict()\n",
    "    for coco_id in coco_ids:\n",
    "        indices = retrieve_for_candidate(scores, idx_mapper, coco_id, from_modality, n)\n",
    "        if from_modality == 'image':\n",
    "            samples = [encoder.decode(sample) for sample in texts[indices]]\n",
    "            result_dict[coco_id] = imgs[idx_mapper[coco_id]], samples\n",
    "        else:\n",
    "            samples = [sample for sample in imgs[indices]]\n",
    "            result_dict[coco_id] = texts[idx_mapper[coco_id]], samples\n",
    "    \n",
    "    plot_matches(result_dict, from_modality, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "dm = COCOCaptionsDataModule(**coco_args)\n",
    "dm.prepare_data()\n",
    "dm.setup('test')\n",
    "dataloaders = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SHRePreTrainingLightningModule.load_from_checkpoint(model_path).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, coco_to_id, imgs, texts = zero_shot_retrieval(model, dataloaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ids = []\n",
    "from_modality = 'image'\n",
    "n_to_retrieve = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_retrievals(scores, coco_to_id, coco_ids, imgs, texts, from_modality, n_to_retrieve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
