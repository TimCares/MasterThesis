{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datamodules import REGISTRY as DATAMODULE_REGISTRY\n",
    "from datamodules import BaseDataModule\n",
    "from rich.progress import track\n",
    "from utils import load_pretrained_d2v_model\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_norm_and_average(target_layer_results:List[torch.Tensor]) -> torch.Tensor:\n",
    "    target_layer_results = [\n",
    "        F.instance_norm(tl.transpose(1, 2).float()).transpose(1, 2)\n",
    "        for tl in target_layer_results  # BTC -> BCT\n",
    "    ]\n",
    "\n",
    "    # clone() -> only the first time step is actually retained, so we not just change the view: https://pytorch.org/docs/stable/notes/serialization.html#saving-loading-tensors\n",
    "    y = target_layer_results[0][:, 0, :].clone().float()\n",
    "    for tl in target_layer_results[1:]:\n",
    "        y.add_(tl[:, 0, :].clone().float())\n",
    "    y = y.div_(len(target_layer_results))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = {\n",
    "    '_name': 'openwebtext',\n",
    "    'model_state_dict': 'nlp_base.pt',\n",
    "    'data_path': '../data',\n",
    "    'out_path': '../data',\n",
    "    'batch_size': 256,\n",
    "    'num_workers': 4,\n",
    "    'shuffle': False,\n",
    "    'drop_last': False,\n",
    "    'num_max_bpe_tokens': 512,\n",
    "    'sample_break_mode': 'none',\n",
    "    \"offset\": 0,\n",
    "}\n",
    "cfg = OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_targets(cfg: DictConfig) -> None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f'Running with device: {device}')\n",
    "\n",
    "    datamodule_kwargs = OmegaConf.to_container(cfg.datamodule)\n",
    "    datamodule_name = datamodule_kwargs.pop('_name', None)\n",
    "    model_state_dict_name = datamodule_kwargs.pop('model_state_dict', None)\n",
    "\n",
    "    model = load_pretrained_d2v_model(state_dict_path=os.path.join('..', 'models', model_state_dict_name))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if datamodule_name is None:\n",
    "        raise ValueError('Field \"_name\" of cfg.datamodule either missing or is None!')\n",
    "    \n",
    "    datamodule_cls = DATAMODULE_REGISTRY[datamodule_name]\n",
    "    \n",
    "    datamodule:BaseDataModule = datamodule_cls(**datamodule_kwargs)\n",
    "\n",
    "    logger.info('Setting up dataset and dataloader...')\n",
    "\n",
    "    datamodule.prepare_data()\n",
    "\n",
    "    datamodule.setup(stage='fit')\n",
    "\n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "\n",
    "    dir_name = f'kd_{datamodule_name}'\n",
    "\n",
    "    kd_targets_path = os.path.join(cfg.datamodule.data_path, dir_name)\n",
    "\n",
    "    os.makedirs(kd_targets_path, exist_ok=True)\n",
    "\n",
    "    batch_idx_offset = cfg.datamodule.offset\n",
    "    id_offset = cfg.datamodule.offset*cfg.datamodule.batch_size\n",
    "\n",
    "    index_items = []\n",
    "    key = None\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in track(enumerate(train_dataloader), description=\"Running predictions...\", total=len(train_dataloader)):\n",
    "            key = batch['modes'][0].name.lower() if key is None else key\n",
    "\n",
    "            padding_mask = batch['padding_mask'] if 'padding_mask' in batch else None \n",
    "            pred = model.extract_features(\n",
    "                source=batch[key].to(device),\n",
    "                mode=None, # determined automatically in model\n",
    "                padding_mask=padding_mask.to(device),\n",
    "                mask=False, # we are creating targets from a teacher model for the student model, so no mask\n",
    "                remove_extra_tokens=False,\n",
    "            )\n",
    "            id = id+batch_idx_offset\n",
    "            filename = f'{idx}_{id_offset+batch[\"id\"][0]}-{id_offset+batch[\"id\"][-1]}.pt'\n",
    "            index_items.append({\n",
    "                \"path\": os.path.join(dir_name, filename),\n",
    "                \"batch_idx\": idx,\n",
    "            })\n",
    "\n",
    "            pred.pop('x', None) # output of final layer not interesting, also, it is contained in 'layer_results' at [-1]\n",
    "            pred.pop('mask', None) # is non here, as we do not mask the kd targets\n",
    "            # pred in now dict with keys \"padding_mask\" and \"layer_results\"\n",
    "            pred['layer_results'] = instance_norm_and_average(pred['layer_results']).cpu()\n",
    "            pred['padding_mask'] = pred['padding_mask'].cpu()\n",
    "\n",
    "            item = {\n",
    "                'target': pred,\n",
    "                key: batch[key],\n",
    "                'modes': batch['modes'],\n",
    "            }\n",
    "            if padding_mask is not None:\n",
    "                item['padding_mask'] = padding_mask\n",
    "            torch.save(item, os.path.join(kd_targets_path, filename))\n",
    "\n",
    "    index = {\n",
    "        'datamodule': OmegaConf.to_container(cfg.datamodule),\n",
    "        'model_state_dict': cfg.model_state_dict,\n",
    "        'index': index_items,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(kd_targets_path, 'index.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(index, f)\n",
    "\n",
    "    logger.info(f'Knowledge-Distillation (KD) targets successfully created under: {kd_targets_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_targets(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
