{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from data.unimodal import get_raw_librispeech_dataset\n",
    "#from fairseq.examples.data2vec.models import Data2VecAudioModel\n",
    "from fairseq.models.wav2vec import Wav2Vec2Model, Wav2Vec2Config\n",
    "from src.models.data2vec_audio import Data2VecAudioModel, Data2VecAudioConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairseq model padding mask test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_feat_extract_output_lengths(conv_feature_layers, input_lengths: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Computes the output length of the convolutional layers\n",
    "        \"\"\"\n",
    "\n",
    "        def _conv_out_length(input_length, kernel_size, stride):\n",
    "            return torch.floor((input_length - kernel_size) / stride + 1)\n",
    "\n",
    "        conv_cfg_list = conv_feature_layers\n",
    "\n",
    "        for i in range(len(conv_cfg_list)):\n",
    "            input_lengths = _conv_out_length(\n",
    "                input_lengths, conv_cfg_list[i][1], conv_cfg_list[i][2]\n",
    "            )\n",
    "\n",
    "        return input_lengths.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.tensor([\n",
    "    [False, False, False, False, False, False, False, False, False, False, False, False],\n",
    "    [False, False, False, False, False, False, False, False, True, True,True, True]\n",
    "])\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths= (1 - padding_mask.long()).sum(-1)\n",
    "conv_feature_layers = [[512, 3, 1], [512, 2, 1], [512, 2, 1], [512, 2, 1], [512, 2, 1]]\n",
    "output_lengths=_get_feat_extract_output_lengths(conv_feature_layers, input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = torch.zeros(2, 10)\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask[\n",
    "                (\n",
    "                    torch.arange(padding_mask.shape[0]),\n",
    "                    output_lengths - 1,\n",
    "                )\n",
    "            ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = (1 - padding_mask.flip([-1]).cumsum(-1).flip([-1])).bool()\n",
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Wav2Vec2ConfigSmall(Wav2Vec2Config):\n",
    "    encoder_layers: int = field(\n",
    "        default=1, metadata={\"help\": \"num encoder layers in the transformer\"}\n",
    "    )\n",
    "    encoder_embed_dim: int = field(\n",
    "        default=128, metadata={\"help\": \"encoder embedding dimension\"}\n",
    "    )\n",
    "    encoder_ffn_embed_dim: int = field(\n",
    "        default=256, metadata={\"help\": \"encoder embedding dimension for FFN\"}\n",
    "    )\n",
    "    encoder_attention_heads: int = field(\n",
    "        default=1, metadata={\"help\": \"num encoder attention heads\"}\n",
    "    )\n",
    "    conv_feature_layers: str = field(\n",
    "        default=\"[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]\",\n",
    "        metadata={\n",
    "            \"help\": \"string describing convolutional feature extraction layers in form of a python list that contains \"\n",
    "            \"[(dim, kernel_size, stride), ...]\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timcares/miniforge3/envs/mmrl/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2Model(cfg=Wav2Vec2ConfigSmall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4613504"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
