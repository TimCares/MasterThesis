{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-24 11:54:12,084] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/MasterThesis/src/visualizations/../beit2/modeling_finetune.py:473: UserWarning: Overwriting beit_base_patch16_224 in registry with modeling_finetune.beit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/root/MasterThesis/src/visualizations/../beit2/modeling_finetune.py:494: UserWarning: Overwriting beit_base_patch16_384 in registry with modeling_finetune.beit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_base_patch16_384(pretrained=False, **kwargs):\n",
      "/root/MasterThesis/src/visualizations/../beit2/modeling_finetune.py:510: UserWarning: Overwriting beit_large_patch16_224 in registry with modeling_finetune.beit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/root/MasterThesis/src/visualizations/../beit2/modeling_finetune.py:519: UserWarning: Overwriting beit_large_patch16_384 in registry with modeling_finetune.beit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_384(pretrained=False, **kwargs):\n",
      "/root/MasterThesis/src/visualizations/../beit2/modeling_finetune.py:528: UserWarning: Overwriting beit_large_patch16_512 in registry with modeling_finetune.beit_large_patch16_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_512(pretrained=False, **kwargs):\n",
      "2024-08-24 11:54:13 | INFO | datasets | PyTorch version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../beit2')\n",
    "from datamodules import DATAMODULE_REGISTRY\n",
    "from models import MODEL_REGISTRY\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "from rich.progress import track\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "plt.rcParams[\"axes.axisbelow\"] = False\n",
    "matplotlib.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embeddings(model, dataloader, device):\n",
    "    img_embeds = []\n",
    "    text_embeds = []\n",
    "    img_ids = []\n",
    "    images = []\n",
    "    texts = []\n",
    "\n",
    "    for batch in track(dataloader):\n",
    "        image = batch['image'].to(device)\n",
    "        text = batch['text'].to(device)\n",
    "        padding_mask = batch['padding_mask'].to(device) if 'padding_mask' in batch else None\n",
    "        # encoding also normalizes the output\n",
    "        img_emb = model.encode_image(image=image)['x']\n",
    "        text_emb = model.encode_text(text=text, padding_mask=padding_mask)['x']\n",
    "        img_embeds.append(img_emb)\n",
    "        text_embeds.append(text_emb)\n",
    "        img_ids.append(batch['id'].to(device))\n",
    "        images.append(batch['image_raw'])\n",
    "        texts.append(text)\n",
    "\n",
    "    return img_embeds, text_embeds, images, texts, img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(img_embeds, text_embeds, images, texts, img_ids):\n",
    "    image_feats = {} # collect all unique image features, and create mapping based on id\n",
    "    raw_images = {}\n",
    "    for feats, ids, img in zip(img_embeds, img_ids, images):\n",
    "        for i, _idx in enumerate(ids):\n",
    "            idx = _idx.item()\n",
    "            if idx not in image_feats:\n",
    "                image_feats[idx] = feats[i]\n",
    "                raw_images[idx] = img[i]\n",
    "\n",
    "    tiids = torch.cat(img_ids, dim=0)\n",
    "    iids = []\n",
    "    sorted_tensors = []\n",
    "    sorted_images = []\n",
    "    for key in sorted(image_feats.keys()):\n",
    "        sorted_tensors.append(image_feats[key].view(1, -1))\n",
    "        sorted_images.append(raw_images[key])\n",
    "        iids.append(key)\n",
    "\n",
    "    img_embeds = torch.cat(sorted_tensors, dim=0)\n",
    "    images = torch.cat(sorted_images, dim=0)\n",
    "    text_embeds = torch.cat(text_embeds, dim=0)\n",
    "\n",
    "    scores = img_embeds @ text_embeds.t()\n",
    "    iids = torch.LongTensor(iids).to(scores.device)\n",
    "\n",
    "    return scores, images, texts, iids, tiids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_retrievals(n_queries, n_retrievals, scores, images, text, iids, tiids):\n",
    "    num_cols = n_retrievals+2\n",
    "    num_rows = len(n_queries)\n",
    "\n",
    "    query_indices = torch.randperm((scores.shape[0]))[:n_queries]\n",
    "\n",
    "    topk_retrievals = scores.topk(n_retrievals, dim=1).indices\n",
    "    \n",
    "    topk_retrievals = topk_retrievals[query_indices]\n",
    "    query_coco_ids = iids[query_indices]\n",
    "    query_samples = images[query_indices]\n",
    "\n",
    "    retrieved_samples = []\n",
    "    retrieval_coco_ids = []\n",
    "    for idx in topk_retrievals:\n",
    "        retrieved_samples.append(text[idx])\n",
    "        retrieval_coco_ids.append(tiids[idx])\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(18, 3*num_rows))\n",
    "\n",
    "    axes[0, 0].set_title(\"COCO ID\")\n",
    "    axes[0, 1].set_title(\"Query\")\n",
    "    for j in range(2, num_cols):\n",
    "        axes[0, j].set_title(f\"Retrieval {j+1}\")\n",
    "\n",
    "    for i, idx, sample in enumerate(zip(query_coco_ids, query_samples)):\n",
    "        axes[i, 0].text(0.5, 0.5, idx.item(), ha='center', va='center', fontsize=12)\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(sample.permute(1, 2, 0))\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    for i, indices, samples in enumerate(zip(retrieval_coco_ids, retrieved_samples)):\n",
    "        samples = tokenizer.batch_decode(samples, skip_special_tokens=True)\n",
    "        for j in range(n_retrievals):\n",
    "            axes[i, j].text(0.5, 0.5, samples[j], ha='center', va='center', fontsize=10)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "            if indices[j].item() == query_coco_ids[i].item():\n",
    "                color = 'green'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            for spine in axes[i, j].spines.values():\n",
    "                spine.set_edgecolor(color)\n",
    "                spine.set_linewidth(2)\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_retrievals(n_queries, n_retrievals, scores, images, text, iids, tiids):\n",
    "    num_cols = n_retrievals+2\n",
    "    num_rows = len(n_queries)\n",
    "\n",
    "    scores = scores.t()\n",
    "\n",
    "    query_indices = torch.randperm((scores.shape[0]))[:n_queries]\n",
    "\n",
    "    topk_retrievals = scores.topk(n_retrievals, dim=1).indices\n",
    "    \n",
    "    topk_retrievals = topk_retrievals[query_indices]\n",
    "    query_coco_ids = tiids[query_indices]\n",
    "    query_samples = text[query_indices]\n",
    "\n",
    "    retrieved_samples = []\n",
    "    retrieval_coco_ids = []\n",
    "    for idx in topk_retrievals:\n",
    "        retrieved_samples.append(images[idx])\n",
    "        retrieval_coco_ids.append(iids[idx])\n",
    "\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=(18, 3*num_rows))\n",
    "\n",
    "    axes[0, 0].set_title(\"COCO ID\")\n",
    "    axes[0, 1].set_title(\"Query\")\n",
    "    for j in range(2, num_cols):\n",
    "        axes[0, j].set_title(f\"Retrieval {j+1}\")\n",
    "\n",
    "    query_samples = tokenizer.batch_decode(query_samples, skip_special_tokens=True)\n",
    "    for i, idx, sample in enumerate(zip(query_coco_ids, query_samples)):\n",
    "        axes[i, 0].text(0.5, 0.5, idx.item(), ha='center', va='center', fontsize=12)\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].text(0.5, 0.5, sample, ha='center', va='center', fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    for i, indices, samples in enumerate(zip(retrieval_coco_ids, retrieved_samples)):\n",
    "        for j in range(n_retrievals):\n",
    "            axes[i, j].imshow(samples[j].permute(1, 2, 0))\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "            if indices[j].item() == query_coco_ids[i].item():\n",
    "                color = 'green'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            for spine in axes[i, j].spines.values():\n",
    "                spine.set_edgecolor(color)\n",
    "                spine.set_linewidth(2)\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/workspace/models/cluster.pt\"\n",
    "MODEL_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dm_kwargs = {\n",
    "    'data_path': '/workspace',\n",
    "    'num_max_bpe_tokens': 64,\n",
    "    'color_jitter': None,\n",
    "    'beit_transforms': False,\n",
    "    'crop_scale': [1.0, 1.0],\n",
    "    'batch_size': 256,\n",
    "    'num_workers': 8,\n",
    "    'shuffle': True,\n",
    "    'drop_last': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "coco_dm = DATAMODULE_REGISTRY['coco_captions'](**coco_dm_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-08-24 11:54:20 | INFO | datasets_.base_datasets | [COCOCaptions]: Data already exists under: /workspace/coco\n",
      "2024-08-24 11:54:20 | INFO | datasets_.base_datasets | [COCOCaptions]: Data already exists under: /workspace/coco\n",
      "2024-08-24 11:54:20 | INFO | datasets_.base_datasets | [COCOCaptions]: Data already exists under: /workspace/coco\n",
      "2024-08-24 11:54:24 | INFO | datasets_.base_datasets | [COCOCaptions]: Load 566747 image-text pairs from /workspace/coco/coco_captioning.train.jsonl. \n",
      "2024-08-24 11:54:24 | INFO | datasets_.base_datasets | [COCOCaptions]: Load 25010 image-text pairs from /workspace/coco/coco_captioning.val.jsonl. \n"
     ]
    }
   ],
   "source": [
    "coco_dm.prepare_data()\n",
    "coco_dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = iter(coco_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vq_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model_cls:LightningModule \u001b[38;5;241m=\u001b[39m \u001b[43mMODEL_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvq_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m model:ImageVQ \u001b[38;5;241m=\u001b[39m model_cls\u001b[38;5;241m.\u001b[39mload_from_checkpoint(MODEL_PATH)\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vq_image'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_cls:LightningModule = MODEL_REGISTRY[MODEL_NAME]['module']\n",
    "model = model_cls.load_from_checkpoint(MODEL_PATH).model\n",
    "model = model.to(device)\n",
    "model.requires_grad_(False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, images, texts, iids, tiids = get_scores(*get_embeddings(model, dl, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = iids.unsqueeze(1) == tiids.unsqueeze(0)\n",
    "selected_image_ids = torch.randperm(mask.shape[0])[:1000]\n",
    "\n",
    "selected_indices = []\n",
    "for row in mask[selected_image_ids]:\n",
    "    true_indices = torch.nonzero(row, as_tuple=False).squeeze()\n",
    "    \n",
    "    if true_indices.numel() == 1:\n",
    "        selected_indices.append(true_indices.item())\n",
    "    elif true_indices.numel() > 0:\n",
    "        selected_index = true_indices[torch.randint(0, len(true_indices), (1,))]\n",
    "        selected_indices.append(selected_index.item())\n",
    "    else:\n",
    "        raise ValueError(\"No matching indices found\")\n",
    "\n",
    "selected_indices = torch.tensor(selected_indices)\n",
    "\n",
    "selected_texts = texts[selected_indices]\n",
    "selected_images = images[selected_image_ids]\n",
    "iids_ = tiids_ = torch.arange(selected_images.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
