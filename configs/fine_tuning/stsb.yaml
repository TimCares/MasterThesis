# @package _global_

hydra:
  run:
    dir: .

defaults:
  - _self_ # this first, so that the base config can be overridden by others
  
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

base_dir: /workspace
model_path: ${base_dir}/models
data_path: ${base_dir}
log_dir: ${base_dir}/logs
seed: 42

name: base
run_name: glue_stsb_finetuning_${name}

load_checkpoint:

metrics:
  - pearson_and_spearman

regression: True

model:
  model_version: 2024-05-20_17-11-53
  num_classes: 1

data:
  dataset: stsb_glue
  num_max_bpe_tokens: 196

  data_path: ${data_path}
  batch_size: 16
  num_workers: 1
  shuffle: True
  drop_last: True


checkpoint:
  dirpath: "${model_path}/${now:%Y-%m-%d_%H-%M-%S}"
  # monitor key is also computed in a callback, ModelCheckpoint callbacks are the last checkpoints executed,
  # so the key will be computed right beforehand (therefore it is always up to date)
  filename: mm-d2v-glue-stsb-{step}-{${checkpoint.monitor}:.4f}
  save_last: link # save last checkpoint -> easier access, as it has a fixed name (last.ckpt) -> not set to True here, as a link saves space
  enable_version_counter: False # only ever one "last.ckpt" file
  every_n_epochs: 1
  save_on_train_epoch_end: False # False -> run at end of validation
  verbose: True
  monitor: val/pearson_and_spearman
  mode: max
  auto_insert_metric_name: False

optimizer:
  lr: 4e-5
  betas: [0.9,0.98]
  eps: 1e-06
  weight_decay: 0.1
  warmup: True

optimizer_schedule:
  warmup_steps: 214
  max_steps: ${lightning_trainer.max_steps}

lightning_trainer:
  default_root_dir: ${log_dir}
  accelerator: gpu
  devices: -1
  strategy: auto # auto -> default
  max_epochs: 10
  max_steps: 3598
  num_sanity_val_steps: 0
  precision: 16-mixed
  log_every_n_steps: 50
