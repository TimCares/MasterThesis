# @package _global_

hydra:
  run:
    dir: .

defaults:
  - _self_ # this first, so that the base config can be overridden by others
  
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

dry_run: False
data_path: ../data
model_path: ../models
seed: 42

load_checkpoint:

nlp_context_length: 512

model:
  pretrained_path: ${model_path}
  pretrained:
    audio: base_libri.pt
    image: base_imagenet.pt
    text: nlp_base.pt

  encoders_embed_dim: 768
  embed_dim: 768
  depth: 5
  num_heads: 8 
  mlp_ratio: 3.0
  encoder_dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.1
  post_mlp_drop: 0.1
  norm_eps: 1e-6
  norm_affine: True
  layer_norm_first: False
  loss_scale: # Set to None -> Scaled by 1 / sqrt(embed_dim) (done for audio and image but not for text in the original implementation)
  dropout_input: 0.0
  start_drop_path_rate: 0
  end_drop_path_rate: 0
  layerdrop: 0.0

  end_of_block_targets: False

  modality_encoder_proj: False

  seed: ${seed}

data:
  # datasets/dataloaders must start with an underscore, the string after that is used as a key to the registries
  data_path: ${data_path}
  # batch_size: 256
  num_workers: 5
  shuffle: True
  drop_last: True

  _kd_datamodule:
    - dataset: openwebtext # openwebtext_at

    - dataset: imagenet # imagenet_at

    - dataset: librispeech # librispeech_at

#   _openwebtext:
#     num_max_bpe_tokens: ${nlp_context_length}
#     sample_break_mode: none
#  
#   _imagenet:
#     beit_transforms: False
#     no_transform: False
#     transform_jitter: False
#     precompute_mask_config: 
#     crop_scale: [0.6, 1]
#     local_cache_path:
#  
#   _librispeech:
#     sample_rate: 16000
#     max_sample_size: 160000
#     min_sample_size: 32000
#     precompute_mask_config: 
#     type_train: 
#     type_test: 

zero_shot_val:
  n_neighbors: 100
  data_path: ${data_path}
  num_max_bpe_tokens: ${nlp_context_length}
  is_multimodal_aligned: False
  val_every_n_batches: 100
  datamodules: # dataloder args are taken from top-level key "data"
    _imdb:
      data_path: ${data_path}
      num_max_bpe_tokens: ${nlp_context_length}
      batch_size: 256
    _cifar10:
      data_path: ${data_path}
      batch_size: 256
    _cifar100:
      data_path: ${data_path}
      batch_size: 256
    _speechcommands:
      data_path: ${data_path}
      min_sample_size: 0 # take all samples
      normalize: True
      pad: True
      batch_size: 256

checkpoint:
  dirpath: "${model_path}/${now:%Y-%m-%d_%H-%M-%S}"
  # monitor key is also computed in a callback, ModelCheckpoint callbacks are the last checkpoints executed,
  # so the key will be computed right beforehand (therefore it is always up to date)
  filename: mm-d2v-{step}-{${checkpoint.monitor}:.4f}
  save_last: link # save last checkpoint -> easier access, as it has a fixed name (last.ckpt) -> not set to True here, as a link saves space
  enable_version_counter: False # only ever one "last.ckpt" file
  every_n_train_steps: ${zero_shot_val.val_every_n_batches}
  save_on_train_epoch_end: False # False -> run at end of validation
  verbose: True
  monitor: val/unimodal-mean-knn--zeroshot-top1-acc # logged in: ZeroShotCallback
  mode: max
  auto_insert_metric_name: False

optimizer:
  lr: 0.00075 # -> 7.5e-4
  betas: [ 0.9,0.98 ]
  eps: 1e-06
  weight_decay: 0.01

optimizer_schedule:
  warmup_steps: 1_000
  max_steps: ${lightning_trainer.max_steps}

lightning_trainer:
  accelerator: gpu
  devices: -1
  #strategy: ddp # -> automatically used if multiple GPUs are available -> devices: -1
  max_steps: 30_000
  val_check_interval: # None -> no validation
  num_sanity_val_steps: 0 # mean test running "num_sanity_val_steps" batches of the validation dataloaders, but here we have no classical validation
  limit_val_batches: 0 # no classical validation
  precision: 16-mixed
  gradient_clip_val: 4.0 # can be a little higher, as some parts are pre-trained(?) DistillBERT has 5.0