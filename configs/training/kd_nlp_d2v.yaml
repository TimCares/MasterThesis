# @package _global_

hydra:
  run:
    dir: .

defaults:
  - _self_ # this first, so that the base config can be overridden by others
  
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

dry_run: False
base_dir: "/workspace"
model_path: ${base_dir}/models
data_path: ${base_dir}
log_dir: ${base_dir}/logs
seed: 42

run_name: nlp_196_img_attn_init_w_val

load_checkpoint:

nlp_context_length: 196

model:
  pretrained_path: ${model_path}
  pretrained:
    audio: base_libri.pt
    image: base_imagenet.pt
    text: nlp_base.pt

  supported_modalities:
    # - AUDIO
    # - IMAGE
    - TEXT

  block_init_cfg:
    init_from: IMAGE
    # Only relevant if "init_from" not None: If "block_indices" is None,
    # 'attention' -> only attention layers are initialized from pretrained,
    # if 'block', then whole Transformer blocks are initialized
    init_type: attention
    # Only relevant if "init_from" not None: If "block_indices" is None,
    # then all init blocks (can also be just attention layers) are initialized
    block_indices:
    # Only relevant if "init_from" not None: If "block_indices" is None,
    # If None, then all init blocks (can also be just attention layers) are frozen,
    # if empty list, then no blocks are frozen
    freeze_blocks: []

  mask: False # if set to False we do normal KD of all the layer activation and timesteps (not just the masked ones)
  # if True we do MaskedKD

  # relevant if "mask" is True:
  inverse_masking: False # if True, then the student gets the masked input, and the saliency score is computed from the teacher output
  frac_keep_tokens: 0.6
  norm_first: False # whether to normalize along all timesteps (True), or only the ones that are kept (False)
  final_attn_layer_saliency_score: False # whether to use the final attention layer saliency score for masking (True) or the average of all layers (False)

  embed_dim: 768

  depth: 6
  num_heads: 12 
  mlp_ratio: 4.0
  encoder_dropout: 0.0
  dropout_input: 0.0
  attention_dropout: 0.0
  activation_dropout: 0.0
  post_mlp_drop: 0.0
  norm_eps: 1e-6
  norm_affine: True
  layer_norm_first: False
  start_drop_path_rate: 0
  end_drop_path_rate: 0
  layerdrop: 0.0

  seed: ${seed}

data:
  dataloader:
    data_path: ${data_path}
    batch_size: 256
    num_workers: 5
    shuffle: True
    drop_last: True

  datamodules:
    openwebtext:
      num_max_bpe_tokens: ${nlp_context_length}
      sample_break_mode: none

checkpoint:
  dirpath: "${model_path}/${now:%Y-%m-%d_%H-%M-%S}"
  # monitor key is also computed in a callback, ModelCheckpoint callbacks are the last checkpoints executed,
  # so the key will be computed right beforehand (therefore it is always up to date)
  filename: nlp-d2v-{step}-{${checkpoint.monitor}:.4f}
  save_last: link # save last checkpoint -> easier access, as it has a fixed name (last.ckpt) -> not set to True here, as a link saves space
  enable_version_counter: False # only ever one "last.ckpt" file
  every_n_train_steps: 6_001 # save after every validation run (else checkpointing is run first, ... bug?)
  save_on_train_epoch_end: False # False -> run at end of validation
  verbose: True
  monitor: val/loss
  mode: min
  auto_insert_metric_name: False

optimizer:
  lr: 0.0005
  betas: [ 0.9,0.98 ]
  eps: 1e-06
  weight_decay: 0.01
  warmup: True

optimizer_schedule: # relevant if "optimizer.warmup" is True
  type: cosine # constant or cosine
  warmup_steps: 6_000
  max_steps: ${lightning_trainer.max_steps}

lightning_trainer:
  default_root_dir: ${log_dir}
  accelerator: gpu
  devices: -1
  strategy: deepspeed_stage_2
  max_steps: 60_000
  val_check_interval: 6_000
  precision: 16-mixed
  log_every_n_steps: 50
