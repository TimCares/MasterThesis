# @package _global_

hydra:
  run:
    dir: .

defaults:
  - _self_ # this first, so that the base config can be overridden by others
  
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

dry_run: False
base_dir: "/workspace"
model_path: ${base_dir}/models
data_path: ${base_dir}
log_dir: ${base_dir}/logs
seed: 42

run_name: SHRe_1e-4_2xscale_dropout_0.1_vg
model_name: SHRe

load_checkpoint:

nlp_context_length: 64 # inspired by BEiT3

model:
  pretrained_path: ${model_path}
  pretrained:
    audio: base_libri.pt
    image: base_imagenet.pt
    text: nlp_base.pt

  embed_dim: 768

  depth: 7
  mlp_ratio: 4.0
  norm_eps: 1e-6
  norm_affine: True

data:
  dataloader:
    batch_size: 256
    num_workers: 3 # each loader has 3 workers -> 6 in total
    shuffle: True
    drop_last: True

  common: # keys used for all datasets
    data_path: ${data_path}
    num_max_bpe_tokens: ${nlp_context_length}
    color_jitter: # None
    beit_transforms: False
    crop_scale: [0.6, 1.0]

  datamodules:
    coco_captions:
      task: captioning

    visual_genome:
      concat_captions: False

    conceptual_captions: {}

checkpoint:
  common:
    dirpath: ${model_path}/${run_name}
    enable_version_counter: False
    every_n_train_steps: 6_001 # save after every validation run (else checkpointing is run first, ... bug?)
    verbose: True
    auto_insert_metric_name: False

  checkpoints:
    - monitor: val/loss
      mode: min
      filename: model-{step}-{${.monitor}:.4f}-val

    - monitor: train/loss
      mode: min
      filename: model-{step}-{${.monitor}:.4f}-train

zero_shot_val:
  val_every_n_batches: 6_000
  num_max_bpe_tokens: ${nlp_context_length}

optimizer:
  lr: 0.0001
  betas: [ 0.9,0.98 ]
  eps: 1e-06
  weight_decay: 0.01
  warmup: True

optimizer_schedule: # relevant if "optimizer.warmup" is True
  type: cosine # constant or cosine
  warmup_steps: 7_000
  max_steps: ${lightning_trainer.max_steps}

lightning_trainer:
  default_root_dir: ${log_dir}
  accelerator: gpu
  devices: -1
  max_steps: 70_000
  val_check_interval: ${zero_shot_val.val_every_n_batches}
  num_sanity_val_steps: 0 # would run zero shot at the beginning if > 0
  precision: 16-mixed
  log_every_n_steps: 50

  deepspeed:
    stage: 2
    offload_optimizer: False
    allgather_bucket_size: 5e8 # size as recommended by pytorch lightning deepspeed docs
    reduce_bucket_size: 5e8 # size as recommended by pytorch lightning deepspeed docs

# memory_bank:
#   batch_size: ${data.dataloader.batch_size}
#   size: 16384
#   device: cuda
#   half_precision: True
