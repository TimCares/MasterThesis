= Appendix
== Hyperparameters <hyperparameters>

#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: 3,
    stroke: none,
    table.hline(),
    table.header(
      [*Type*],
      [*Hyperparameters*],
      [*Values*],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 6, align:horizon, [*Model*]), 
    [Layers], [6],
    [Hidden size], [768],
    [FFN inner hidden size], [3072],
    [Attention Heads], [12],
    [Patch size], [16$times$16],
    [Input resolution], [224$times$224],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 11, align:horizon, [*Training*]), 
    [Epochs], [10],
    [Total steps], [50040],
    [Batch size], [256],
    [Optimizer], [AdamW],
    [AdamW $epsilon$], [1e-06],
    [AdamW $beta$], [(0.9,0.98)],
    [Weight decay], [0.01],
    [Base learning rate], [1e-4],
    [Learning rate schedule], [Cosine],
    [Warmup steps], [5004 (10% of total steps)],
    [Hardware], [1 $times$ RTX 4090 24GB],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 2, align:horizon, [*Augmentations*]), 
    [Horizontal flipping prob.], [0.5],
    [RandomResizeCrop range], [[0.08, 1.0]],
    table.hline(),
  ),
  caption: [Hyperparameters used for distilling a Data2Vec2 image model.
  ],
)<distil_data2vec2_hyperparameters>


#show table: set text(8pt)
#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: (auto, auto, auto, auto, auto, auto, auto, auto),
    stroke: none,
    table.hline(),
    table.header(
      table.cell(rowspan: 2, align:horizon, [*Type*]),
      table.cell(rowspan: 2, align:horizon, [*Hyperparameters*]),
      table.cell(colspan: 2, align:horizon, [*ImageNet*]),
      table.cell(colspan: 2, align:horizon, [*CIFAR10*]),
      table.cell(colspan: 2, align:horizon, [*CIFAR100*]),
      [Finetune], table.vline(stroke: .3pt), [Linear probe],table.vline(stroke: .3pt), 
      [Finetune], table.vline(stroke: .3pt), [Linear probe],table.vline(stroke: .3pt), 
      [Finetune], table.vline(stroke: .3pt), [Linear probe],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 12, align:horizon, [*Training*]), 
    [Num classes], [1k], [1k], [10], [10], [100], [100],
    [Epochs], table.cell(colspan: 6, align:horizon, [15]),
    [Batch size], table.cell(colspan: 6, align:horizon, [256]),
    [Optimizer], table.cell(colspan: 6, align:horizon, [AdamW]),
    [AdamW $epsilon$], table.cell(colspan: 6, align:horizon, [1e-8]),
    [AdamW $beta$], table.cell(colspan: 6, align:horizon, [(0.9, 0.999)]),
    [Weight decay], table.cell(colspan: 6, align:horizon, [0.01]),
    [Base learning rate], table.cell(colspan: 6, align:horizon, [1e-3]),
    [Layer Decay], [0.81], [-], [0.75], [-], [0.75], [-],
    [Learning rate schedule], table.cell(colspan: 6, align:horizon, [Cosine]),
    [Warmup steps], table.cell(colspan: 6, align:horizon, [10% of total steps]),
    [Hardware], table.cell(colspan: 6, align:horizon, [1 $times$ RTX 4090 24GB]),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 5, align:horizon, [*Mixup* @mixup*\/\ Cutmix* @cutmix]),
    [Mixup prob.], table.cell(colspan: 6, align:horizon, [0.8]),
    [Cutmix prob.], table.cell(colspan: 6, align:horizon, [1.0]),
    [Prob.], table.cell(colspan: 6, align:horizon, [0.9]),
    [Switch prob.], table.cell(colspan: 6, align:horizon, [0.5]),
    [Label smooting], table.cell(colspan: 6, align:horizon, [0.1]),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 4, align:horizon, [*RandAugment* @randaugment]),
    [Magintude], table.cell(colspan: 6, align:horizon, [9]),
    [Magnitude std.], table.cell(colspan: 6, align:horizon, [0.5]),
    [Magnitude inc.], table.cell(colspan: 6, align:horizon, [1]),
    [\# ops], table.cell(colspan: 6, align:horizon, [2]),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 3, align:horizon, [*RandomErase* @randerase]),
    [Prob.], table.cell(colspan: 6, align:horizon, [0.25]),
    [Mode], table.cell(colspan: 6, align:horizon, [pixel]),
    [\# erase], table.cell(colspan: 6, align:horizon, [1]),
    table.hline(),
  ),
  caption: [Hyperparameters used for the ImageNet-1K @imagenet, CIFAR10 @cifar_10_100, and CIFAR100 @cifar_10_100 of the distilled Data2Vec2 image model.
  We refer to the respective papers for details on the augmentation techniques @mixup @cutmix @randaugment @randerase.
  ],
)<distil_data2vec2_imagenet_finetuning_hyperparameters>
#show table: set text(12pt)

#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: 3,
    stroke: none,
    table.hline(),
    table.header(
      [*Type*],
      [*Hyperparameters*],
      [*Values*],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 6, align:horizon, [*Model*]), 
    [Layers], [6],
    [Hidden size], [768],
    [FFN inner hidden size], [3072],
    [Attention Heads], [12],
    [Max sequence length], [256],
    [Vocabulary size], [30522],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 11, align:horizon, [*Training*]), 
    [Epochs], [1],
    [Total steps], [1M],
    [Batch size], [256],
    [Optimizer], [AdamW],
    [AdamW $epsilon$], [1e-06],
    [AdamW $beta$], [(0.9,0.98)],
    [Weight decay], [0.01],
    [Base learning rate], [1e-4],
    [Learning rate schedule], [Cosine],
    [Warmup steps], [10k (1% of total steps)],
    [Hardware], [1 $times$ RTX 4090 24GB],
    table.hline(),
  ),
  caption: [Hyperparameters used for distilling a BERT model.
  ],
)<distil_bert_hyperparameters>

#show table: set text(7pt)
#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: 11,
    stroke: none,
    table.hline(),
    table.header(
      [*Type*],
      [*Hyperparameters*],
      [*MNLI*],
      [*QNLI*],
      [*RTE*],
      [*MRPC*],
      [*QQP*],
      [*STS-B*],
      [*CoLA*],
      [*SST*],
      [*WNLI*],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 14, align:horizon, [*Training*]), 
    [Num classes], [3], [2], [2], [3], [2], [-], [2], [2], [2],
    [Head Dropout prob.], table.cell(colspan: 9, align:horizon, [0.1]),
    [Epochs], [10], [10], [20], [20], [10], [20], [20], [10], [20],
    [Total steps], [61360], [32733], [3113], [5095], [31563], [7187], [10689], [21047], [1588],
    [Batch size], [64], [32], [16], [16], [128], [16], [16], [32], [8],
    [Optimizer], table.cell(colspan: 9, align:horizon, [AdamW]),
    [AdamW $epsilon$], table.cell(colspan: 9, align:horizon, [1e-6]),
    [AdamW $beta$], table.cell(colspan: 9, align:horizon, [(0.9, 0.98)]),
    [Weight decay], table.cell(colspan: 9, align:horizon, [0.1]),
    [Base learning rate], [2e-5], [2e-5], [2e-5], [2e-5], [2e-5], [4e-5], [1e-5], [2e-5], [1e-5],
    [Learning rate\ schedule], table.cell(colspan: 9, align:horizon, [Polynomial decay]),
    [Warmup steps], table.cell(colspan: 9, align:horizon, [10% of total steps]),
    [Metric], [Accuracy], [Accuracy], [Accuracy], [F1], [F1], [Spearman], [Accuracy], [Accuracy], [Accuracy],
    [Hardware], table.cell(colspan: 9, align:horizon, [1 $times$ RTX 4090 24GB]),
    table.hline(),
  ),
  caption: [Hyperparameters for the GLUE @glue benchmark tasks of the distilled BERT model.
  STS-B @stsb is a regression task, and therefore does not have any classes. Here, the head returns a scalar.
  ],
)<distil_bert_glue_finetuning_hyperparameters>
#show table: set text(12pt)

#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: 3,
    stroke: none,
    table.hline(),
    table.header(
      [*Type*],
      [*Hyperparameters*],
      [*Values*],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 8, align:horizon, [*Model*]), 
    [Image/Text layers], [6 (Transformer)],
    [Shared layers], [3 (MLP)],
    [Hidden size], [768],
    [FFN inner hidden size], [3072],
    [Attention Heads], [12],
    [Patch size], [16$times$16],
    [Input resolution], [224$times$224],
    [Max. caption length], [64],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 11, align:horizon, [*Training*]), 
    [Epochs], [7],
    [Total steps], [89273],
    [Batch size], [256],
    [Optimizer], [AdamW],
    [AdamW $epsilon$], [1e-06],
    [AdamW $beta$], [(0.9,0.98)],
    [Weight decay], [0.01],
    [Base learning rate], [1e-4],
    [Learning rate schedule], [Cosine],
    [Warmup steps], [8927 (10% of total steps)],
    [Hardware], [1 $times$ RTX 4090 24GB],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 2, align:horizon, [*Augmentations*]), 
    [Horizontal flipping prob.], [0.5],
    [RandomResizeCrop range], [[0.9, 1.0]],
    table.hline(),
  ),
  caption: [Hyperparameters used for training the Transformer SHRe model.
  ],
)<transformer_shre_hyperparams>

#figure(
  table(
    table.vline(x:1, stroke: .3pt),
    table.vline(x:2, stroke: .3pt),
    columns: 3,
    stroke: none,
    table.hline(),
    table.header(
      [*Type*],
      [*Hyperparameters*],
      [*Values*],
    ),
    table.hline(stroke: .6pt),
    table.cell(rowspan: 6, align:horizon, [*Model*]), 
    [Encoder], [BEiTv2 ViT-B/16],
    [\# Decoder layers], [1],
    [Codebook size], [{1024$times$16, 8192$times$16}],
    [Patch size], [16$times$16],
    [EMA decay], [0.99],
    [Mask prob.], [0.9],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 11, align:horizon, [*Training*]), 
    [Epochs], [10],
    [Total steps], [50040],
    [Batch size], [256],
    [Optimizer], [AdamW],
    [AdamW $epsilon$], [1e-06],
    [AdamW $beta$], [(0.9,0.98)],
    [Weight decay], [0.01],
    [Base learning rate], [1e-3],
    [Learning rate schedule], [Cosine],
    [Warmup steps], [5004 (10% of total steps)],
    [Hardware], [2 $times$ RTX 4090 24GB],
    table.hline(stroke: .6pt),
    table.cell(rowspan: 2, align:horizon, [*Augmentations*]), 
    [Horizontal flipping prob.], [0.5],
    [RandomResizeCrop range], [[0.5, 1.0]],
    table.hline(),
  ),
  caption: [Hyperparameters used for training the image vector quantizer.
  ],
)<image_vq_cls_hparams>