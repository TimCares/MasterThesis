== Vision-Language Contrast <vision_language_contrast>

Introduced as a method for self-supervised learning of image models ((TODO: cite contrastive learning section)), contrastive learning
can be extended from unimodal (image) to multimodal applications, such as image and text.
As mentioned in the previous section, we aim to maximize the cosine similarity between
an image and its corresponding text (i.e., caption), and vice versa.
Augmentation is not needed, as we always have pairs: one image and one text.
Negative samples for images are captions of other images, and vice versa.
In this setting, the model learns to produce similar representations for an image and its caption, describing the same real-world concept,
and dissimilar representations for an image and caption that are unrelated. A conceptual example for both vision and vision-language
contrastive learning can be seen in @contrastive_alignment.

#figure(
  image(
  width: 75%,
  "../figures/contrastive_alignment.png"),
  caption: [Contrastive learning aims to align the same (or similar) real-world concepts in representation space, while pushing different concepts apart. Multimodal contrastive learning (b) requires existing pairs, e.g. image-text, while for the unimodal case (a) pairs are synthetically created by augmenting the input. Images and text in the figure have been taken from the COCO train set @coco.],
) <contrastive_alignment>


Contrastive learning requires a (global) representation of the input, which is then used to compare it with other inputs.
Since the introduction of the vision Transformer in 2020 by Dosovitskiy et al. @vit, most vision-language models
are exclusively based on the Transformer architecture, which is why the $mono("[CLS]")$ token is used as the global representation
for both image ($mono(["I_CLS"])$) and text ($mono(["T_CLS"])$), respectively. There have been other approaches, such as
Cross-Modal Late Interaction introduced in FLILP @filip, but they usually require significantly more compute @filip and
do not outperform global contrastive learning @beit3, which is what we use here.

The representations are generated by passing the image sequence $bold(H)_(v, 0)$ and text sequence $bold(H)_(w, 0)$
through the vision-language model $f$,
and extracting the representations for both tokens ($bold(h)_(v, L, mono(["I_CLS"]))$ and $bold(h)_(w, L, mono(["T_CLS"]))$)
from the output of the final layer $bold(H)_(v, L)$ and $bold(H)_(w, L)$, which is the output of the Transformer.
For the resulting batch of image and text representations
${bold(h)_((v, L, mono(["I_CLS"])), k), bold(h)_((w, L, mono(["T_CLS"])), k)}_(k=1)^B$, where $B$ is the batch size,
the cosine similarity between all possible image-text pairs is computed. The cosine similarity is given by:

$
cos(bold(a), bold(b)) = (bold(a) bold(b)^T) / (||bold(a)||_2 * ||bold(b)||_2) = bold(a)/(||bold(a)||_2) bold(b)^T/(||bold(b)||_2)
$

$bold(a) bold(b)^T$ denotes the simple dot product between both representations. $||bold(a)||_2$ and $||bold(b)||_2$
denote the L2-norm of the representations.

The cosine similarity between all possible image-text pairs can be computed efficiently by organizing all image and text representations
in a matrix, which is already given in a batch-wise training, and normalizing every representation.

$
bold(h)' = bold(h) / (||bold(h)||_2)
$

$
bold(I) = [bold(h)'_((v, L, mono(["I_CLS"])),1), bold(h)'_((v, L, mono(["I_CLS"])),2), ..., bold(h)'_((v, L, mono(["I_CLS"])),B)] in RR^(B times D)
$

$
bold(T) = [bold(h)'_((w, L, mono(["T_CLS"])),1), bold(h)'_((w, L, mono(["T_CLS"])),2), ..., bold(h)'_((w, L, mono(["T_CLS"])),B)] in RR^(B times D)
$

$I$ denotes the batch/matrix of image representations, and $T$ contains the text representations. $D$ is the dimensionality of the representations,
often referred to as the hidden size or hidden dimension in Transformers.

A matrix multiplication of both batches of representations then computes the dot product between every image with every text, and vice versa.
Since the representations are normalized, the result will be the cosine similarity between all possible image-text pairs in the batch.

$
bold(L) = bold(I) bold(T)^T, bold(L) in RR^(B times B)
$ <contrastive_logits>

$bold(L)_(i,j)$ then denotes the similarity between image $i$ and text $j$ in the batch. The diagonal of the matrix contains the similarity
between positive pairs, i.e., the correct image-text pairs $(i,i)$, with $bold(L)_(i, i)$ describing their similarity.
For an image, all other texts in the batch are considered as negative samples, and vice versa for text. The superscript $T$ denotes the transpose
of a matrix, and is not to be confused with the batch of text representations $bold(T)$.

For a batch size of 256 ($B=256$), each image has 255 negative samples (i.e., captions of other images)
and one positive sample (i.e., its own caption), the same holds vice versa. This can be seen as a classification problem with 256 classes,
where the model has to predict the correct class out of 256 classes, and each class representing one caption or image, respectively.
For an image, the logit for the correct class is the similarity (cosine) to its own caption, and the logits for the negative classes
are the similarities to the captions of other images. The same holds vice versa for text.

To calculate the loss, the cross-entropy loss is used. For a batch, the loss for selecting the correct caption for each image is given by:

$
cal(L)_"CL"^("i2t") = 1/B sum_(i=1)^B -log exp(bold(L)_(i, i))/(sum_(k=1)^B exp(bold(L)_(i, k)))
$

$exp(bold(L)_(i, i))/(sum_(k=1)^B exp(bold(L)_(i, k)))$ denotes the softmax-normalized similarity between an image and its correct caption,
which is the usual way for calculating the cross-entropy. The result of this normalization is a probability distribution for each image,
where each caption in the batch has a probability of being the correct caption for the image, and vice versa. The probability that the correct
caption belongs to the current image is then used to calculate the negative log-likelihood, which is the loss.

Accordingly, the loss for selecting the correct image for each caption is given by:

$
cal(L)_"CL"^("t2i") = 1/B sum_(i=1)^B -log exp(bold(L)_(i, i))/(sum_(k=1)^B exp(bold(L)_(k, i)))
$

Here, the softmax-normalization is with respect to the similarity of a text with all other images in the batch. The final loss is the mean
of the image-to-text and text-to-image loss:

$
cal(L)_"CL" = 1/2 * (cal(L)_"CL"^("i2t") + cal(L)_"CL"^("t2i"))
$

Returning to the concept of contrastive learning, this process ensures that the similarity between the representation of an image and its caption
is maximized, i.e. close to each other, while the similarity between an image and an unrelated caption is minimized, i.e. far apart.
Only this would appropriately minimize the loss, and thus the model learns to align the representations of the same concept across modalities.
An illustration of multimodal contrastive learning can be found in @contrastive_learning_fig.

#figure(
  image("../figures/itc.png"),
  caption: [Contrastive Learning is performed using matrix multiplication of normalized representations (1), and the result
  is matrix $bold(L)$ described in @contrastive_logits. The representations are
  given by the $mono(["CLS"])$ token of the respective modality, but are represented as #text(font: "Times New Roman")[I]
  and #text(font: "Times New Roman")[T] in the figure for simplicity. The diagonal of the resulting matrix contains
  the cosine similarity between positive samples. The softmax operation along the rows yields a probabilty distribution for
  each image over all captions, and the softmax operation along the columns vice versa (2). The cross-entropy loss is
  then used to calculate the loss for the distributions. The final loss is the mean of both losses.
  Image-Text pairs in the figure have been taken from the COCO train set @coco.],
) <contrastive_learning_fig>

The performance of contrastive learning is highly dependent on the number of negative samples available, which directly
translates to the batch size.
For instance, with a batch size of two, the model only needs to differentiate between one caption that belongs
to the image and one that does not (a negative sample), and vice versa. This task is significantly simpler than
with 255 negative samples or more, where there might be captions
that are semantically similar to the image, but do not belong to it. So with increased negative samples,
to probability of encountering hard-negative examples increases, forcing the model to aggregate as much information as possible
in $mono(["I_CLS"])$ and $mono(["T_CLS"])$ to even differentiate between semantically similar concepts.

The results improve with an increased number of negative examples @moco @beit3, which 
we will also show later, in the experiments section.
More negative samples are usually achieved by using larger batch sizes @moco @clip @beit3.
However, this typically requires higher VRAM GPUs, or multiple GPUs, which is costly.
