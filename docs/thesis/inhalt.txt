(EMKUM:) Extracting Multimodal Knowledge from Unimodal Models

Introduction
	Motivation
	Goals and Contributions
	Content
Background
	Notations and Definitions
	Transformers
	Knowledge-Distillation
	Self-Supervised Learning
	Contrastive Learning
	Multimodal Models
	Vision-Language Contrast
	Image-Text Retrieval
	Related Work
		Deep Aligned Representations
		CLIP
		VLMo
Methodology
	Tools
    Experimental Approach
	Data Collection and Preparation
		Unimodal Data
		Multimodal Data
		On Curated Datasets
		Data Organization
Experiments
	Unimodal Knowledge Distillation
		Vision
		Language
	Multimodal Knowledge Distillation
		Transformer SHRe
			Baseline
			Multimodal Transformer Block
			Larger Batch Sizes with DDP
		Self-Supervised Teacher
		Contrastive Learning
		Token-Type Embedding
		Object-Level Regression
		Performance Evaluation Across Teacher Models
	Benchmarking on Downsteam Tasks
		Vision
		Language
		Vision-Language
	Ablation Study: Reduction of Data
Discussion of Results
Conclusion
	Outlook
	Lessons Learned
Appendix
	Hyperparameters
	Pseudocode
	Additional Visualizations
	Technical Details

Literaturverzeichnis