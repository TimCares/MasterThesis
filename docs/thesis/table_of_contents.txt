EMKUM: Extracting Multimodal Knowledge from Unimodal Models
BYOML: Bootstrap your own multimodal latent

<Abstract>
Introduction
	Motivation
	Goals and Contributions
	Content
Background
	Notations and Definitions (... of image patches etc. -> if this changes for specific experiments, mention then and redefine)
	(Transformers)
	Knowledge-Distillation
	Self-Supervised Learning
	Contrastive Learning
	Multimodal Models
	Vision-Language Contrast
	Image-Text Retrieval
	Related Work
		Deep Aligned Representations
		CLIP
		VLMo
		BEiT v3
Methodology
	Tools
    Experimental Approach
	Data Collection and Preparation (mention uncurated nature -> imporant for scaling, also mention 
						  that technically it is labeled, but not done on "purpose" -> some humans
						  did is without regarding it as data for ai -> so the data is technically still appearing "in the wild")
Experiments
	Unimodal Knowledge Distillation
		Vision
		Language
	Multimodal Knowledge Distillation
		Transformer SHRe
		Aligned Representations
			Supervised Teacher
				Larger Batch Sizes with DDP
				Multimodal Transformer Block
			Self-Supervised Teacher
			Contrastive Learning
				On Zero-Shot Retrieval
				Short Captions (Region Descriptions)
				Projections for Task-Seperation
				Image-Text Matching with Feature-Fusion
				Increasing Negative Examples for ITC
				
			Token-Type Embedding (Also try without layerscale)
			Unimodal-2-Multimodal Projection
			Target Cross-Modal Late Interaction
				Cross-Modal Late Interaction
				Method
				(Empty Target)
			Object-Level Regression
				Image MSE only
				Contrastive Target Loss (each teacher target is a class -> not 1000 classes, but 256 or 512, each being the cls token of one image passed to the teacher)
				Dimensionality Reduction (lin projection on both, teacher proj in ema of student proj)
				Feature Clusters
				Quantizing Visual Features
			Loss Ablation Studies
				Removing ITC
				Removing KD (Teacher)
			Self-Distillation (Dino Loss)
			Masked Data Modeling as additional Loss (might make other task difficlut though)
			Distillation-less approach (Teacher stays for IBN of MIM)
			Multimodal Teacher
				
			Biased VL Initialization (init from text)
			Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Importance of the right Teacher (why BEiT-2? etc...) (D2V2 Image, BEiT-3, Dino, )
			Non-Knowledge-Distaillation (motivation -> application of kd very limited, especially with unimodal model)
				Remove ITC? and do finetuning?
			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
			Masked Vision-Language Modeling
			Additional Contribution: DistilBEiT-3

		Ablation Study: Reduction of Data (Mulitple reductions and plot retrieval and zero shot)
		
		
Conclusion
Outlook (Same vs. different pos encoding ^*, can architectures be mixed?, add other modalities, especially audio + add audio)
Lessons Learned
Appendix (plots, money spend etc.)
	Hyperparameters
	Pseudocode
	Figures and Visualizations
	Technical Details


^*
Architectural Studies
	Mixed Positional Encodings (defaults with pretrained D2V modules, include Token-Type Embeddings!)
	Shared Positional Encodings