EMKUM: Extracting Multimodal Knowledge from Unimodal Models
BYOML: Bootstrap your own multimodal latent

Introduction
Background
	Self-Supervised Learning
		Masked Modality (Image/Text) Modeling
		Contrastive Learning and Retrieval
		Knowledge-Distillation
	Related Work
		Data2Vec
		Deep Aligned Representations
		CLIP
		FLAVA
		VLMo
		BEiT
Methodology
	Tools
    Experimental Approach
	Data and Preparation (mention uncurated nature -> imporant for scaling)
Experiments
	Notations and Definitions (... of image patches etc. -> if this changes for specific experiments, mention then and redefine)
	Unimodal Knowledge Distillation
	Multimodal Knowledge Distillation
		Reproducing: â€œSee, Hear, Read: Deep Aligned Representations"
		Aligned Representations
			Supervised Teacher
				Larger Batch Sizes with DDP
				Multimodal Transformer Block
			Self-Supervised Teacher
			Contrastive Learning
				On Zero-Shot Retrieval
				Short Captions (Region Descriptions)
				Projections for Task-Seperation
				Image-Text Matching with Feature-Fusion
				Increasing Negative Examples for ITC
				Ablation Study: Removing ITC
				
			Knowledge-Distillation Specific
				Target-CMLI (lin transform of teacher cls token and student cls token needed?) (-log(1-x) for phony target? or use eos?)
				Token-Type Embedding
				Dino Loss
				Masked Vision-Language Modeling (same everything, just tokens randomly removed -> we can reuse teacher activations)
				Dimensionality Reduction (PCA or one learnable conv1d layer/network (cnv1d inspred by paper about feature whitening to align embedding dims))
				Feature Clusters
				(Importance of the right Teacher (why BEiT-2? etc...)) (D2V2 Image, BEiT-3, Dino, )
				(Biased VL Initialization (init from text))
				Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Non-Knowledge-Distaillation
			Architectural Studies
				Mixed Positional Encodings (defaults with pretrained D2V modules, include Token-Type Embeddings!)
				Shared Positional Encodings
			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
			Pretrain-CoCa (image encoder is beit2, maybe tune only upper two layers, unimodal text decoder is 6 layer d2v2 text, multimodal text decoder is 2 from scratch)
			Masked Vision-Language Modeling
			Additional Contribution: DistilBEiT-3

		Ablation Study: Reduction of Data (Mulitple reductions and plot retrieval and zero shot)
		
Outlook
Conclusion
Lessons Learned
Appendix (plots, money spend etc.)
	Hyperparameters
	Additional Visualizations
	Technical Details
