EMKUM: Extracting Multimodal Knowledge from Unimodal Models
BYOML: Bootstrap your own multimodal latent

<Abstract>
Introduction
	Motivation
	Goals and Contributions
	Content
Background
	(Transformers)
	Notations and Definitions (... of image patches etc. -> if this changes for specific experiments, mention then and redefine)
	Knowledge-Distillation
	Self-Supervised Learning
	Contrastive Learning
	Multimodal Models
	Vision-Language Contrast
	Image-Text Retrieval
	Related Work
		Deep Aligned Representations
		CLIP
		VLMo
		BEiT v3
Methodology
	Tools
    Experimental Approach
	Data and Preparation (mention uncurated nature -> imporant for scaling)
Experiments
	Unimodal Knowledge Distillation
		Vision
		Language
	Multimodal Knowledge Distillation
		Reproducing SHRe
		Aligned Representations
			Supervised Teacher
				Larger Batch Sizes with DDP
				Multimodal Transformer Block
			Self-Supervised Teacher
			Contrastive Learning
				On Zero-Shot Retrieval
				Short Captions (Region Descriptions)
				Projections for Task-Seperation
				Image-Text Matching with Feature-Fusion
				Increasing Negative Examples for ITC
				Ablation Study: Removing ITC
				
			Token-Type Embedding
			Unimodal-2-Multimodal Projection
			Target Cross-Modal Late Interaction
				Cross-Modal Late Interaction
				Method
				Empty Target
			Object-Level Regression
				Contrastive Target Loss (each teacher target is a class -> not 1000 classes, but 256 or 512, each being the cls token of one image passed to the teacher)
				Dimensionality Reduction (lin projection on both, teacher proj in ema of student proj)
				Discussing Feature Clusters
			(Latent Space Mapping
				Student2Teacher
				Teacher2Student)
			Knowledge-Distillation Specific
				Dino Loss
				
				(Masked Vision-Language Modeling (same everything, just tokens randomly removed -> we can reuse teacher activations))
				(Biased VL Initialization (init from text))
				Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Importance of the right Teacher (why BEiT-2? etc...) (D2V2 Image, BEiT-3, Dino, )
			Non-Knowledge-Distaillation (motivation -> application of kd very limited, especially with unimodal model)
				Remove ITC? and do finetuning?
			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
			Pretrain-CoCa (image encoder is beit2, maybe tune only upper two layers, unimodal text decoder is 6 layer d2v2 text, multimodal text decoder is 2 from scratch)
			Masked Vision-Language Modeling
			Additional Contribution: DistilBEiT-3

		Ablation Study: Reduction of Data (Mulitple reductions and plot retrieval and zero shot)
		
		
Conclusion
Outlook (Same vs. different pos encoding ^*, can architectures be mixed?, add other modalities, especially audio + add audio)
Lessons Learned
Appendix (plots, money spend etc.)
	Hyperparameters
	Pseudocode
	Figures and Visualizations
	Technical Details


^*
Architectural Studies
	Mixed Positional Encodings (defaults with pretrained D2V modules, include Token-Type Embeddings!)
	Shared Positional Encodings