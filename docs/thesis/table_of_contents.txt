Introduction
Background
	Self-Supervised Learning
		Masked Modality (Image/Text) Modeling
		Contrastive Learning and Retrieval
		Knowledge-Distillation
	Related Work
		Data2Vec
		Deep Aligned Representations
		CLIP
		FLAVA
		VLMo
		BEiT
Methodology
	Tools
    Experimental Approach
	Data and Preparation (mention uncurated nature -> imporant for scaling)
Experiments
	Notations and Definitions (... of image patches etc. -> if this changes for specific experiments, mention then and redefine)
	Unimodal Knowledge Distillation
	Multimodal Knowledge Distillation
		Aligned Representations
			Dual Encoder
				Reproducing: â€œSee, Hear, Read: Deep Aligned Representations"
				Larger Batch Sizes with DDP
				Multimodal Transformer Block
				Self-Supervised Teacher
					Implications of Feature-Based Distillation 
				Applying Contrastive Learnings
					On FLAVA's retrieval performance
					Short Captions (Region Descriptions)
					Projections for Task-Seperation
					Image-Text Matching with Feature-Fusion
					Align before Fuse [(try both, comare performance on retrieval (once using image/text encoder cls, 
					once with shared cls output), test performance of linear eval on imagenet/glue -> is no align before fuse better? Here it makes sense to use seperate VLMo-like projections as the cls output of the encoders are still modality independent)
					Memory Bank for Larger Batch Sizes (mention momentum encoder from MoCo and ALBEF -> we can't use it due to memory constraint + one additional component would make it more complicated)
					Memory Bank vs. DDP (512)
				Feature Whitening
				Unimodal Student
				Stagewise Unimodal Distillation

				Embedding Clusters
				Importance of the right Teacher (why BEiT-2? etc...)
				Biased VL Initialization (init from text)
				Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Mixture-of-Modality Experts
				Baseline
				Mixing Positional Encodings
				Token-Type Embeddings

			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)

		Ablation Study: Reduction of Data
		
Outlook
Conclusion
Lessons Learned
Appendix (plots, money spend etc.)
