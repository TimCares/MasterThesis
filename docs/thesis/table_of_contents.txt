Introduction
Background
	Self-Supervised Learning
		Masked Modality (Image/Text) Modeling
		Contrastive Learning and Retrieval
		Knowledge-Distillation
	Related Work
		Data2Vec
		Deep Aligned Representations
		VLMo
		BEiT
		CLIP
		FLAVA
Methodology
	Tools
    Experimental Approach
	Data and Preparation
Experiments
	Unimodal Knowledge Distillation
	Multimodal Knowledge Distillation
		Aligned Representations
			Dual Encoder
				Reproducing: â€œSee, Hear, Read: Deep Aligned Representations"
				Self-Supervised Teacher
					Implications of Feature-Based Distillation 
				Applying Contrastive Learning
					On FLAVA's retrieval performance
					Short Captions (Region Descriptions)
					Memory Bank for Larger Batch Sizes
					(Larger Batch Sizes with DDP)
					VLMo Contrast vs. SHRe Contrast
				Feature Whitening
				Unimodal Student
				Stagewise Unimodal Distillation

				
				Importance of the right Teacher
			(Biased VL Initialization)
			Mixture-of-Modality Experts
				Baseline
				Mixing Positional Encodings
				Token-Type Embeddings
			Reusing Teacher Activations
			Ablation Study: Reduction of Data
			Ablation Study: Uncurated Datasets (CC12M)

			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
		
Outlook
Conclusion
Lessons Learned
Appendix (plots, money spend etc.)
