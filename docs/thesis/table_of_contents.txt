EMKUM: Extracting Multimodal Knowledge from Unimodal Models
BYOML: Bootstrap your own multimodal latent

<Abstract>
Introduction
	Motivation
	Goals and Contributions
	Content
	Required Background Knowledge
Background
	Basic Loss Functions
	Knowledge-Distillation
	Transformer
		Language
		Vision
		Vision-Language
	Self-Supervised Learning
		Motivation
		Contrastive Learning
			Vision
			Vision-Language
			Vision-Language Retrieval
	Related Work
		Deep Aligned Representations
		CLIP
		VLMo
		BEiT v3
Methodology
	Tools
    Experimental Approach
	Data Collection and Preparation (mention uncurated nature -> imporant for scaling, also mention 
						  that technically it is labeled, but not done on "purpose" -> some humans
						  did is without regarding it as data for ai -> so the data is technically still appearing "in the wild")
Experiments
	Unimodal Knowledge Distillation
		Vision
		Language
	Multimodal Knowledge Distillation
		Transformer SHRe (imagenet not zero-shot!!!)
		Aligned Representations
			Supervised Teacher
				Larger Batch Sizes with DDP
				Multimodal Transformer Block (Mention increased model size)
			Self-Supervised Teacher
			Contrastive Learning
				On Zero-Shot Retrieval
				Short Captions (Region Descriptions)
				Projections for Task-Seperation
				Image-Text Matching with Feature-Fusion
				Increasing Negative Examples for ITC
				
			Token-Type Embedding (Also try without layerscale)
			Unimodal-2-Multimodal Projection
			Target Cross-Modal Late Interaction
				Cross-Modal Late Interaction
				Method
				(Empty Target)
			Unimodal Masking
			Object-Level Regression
				Image MSE only
				Contrastive Target Loss (each teacher target is a class -> not 1000 classes, but 256 or 512, each being the cls token of one image passed to the teacher)
				Dimensionality Reduction (lin projection on both, teacher proj in ema of student proj)
				Feature Clusters
				Quantizing Global Features
					CLS Token
					Patch Aggregation
					Codebook Collapse
			Loss Ablation Studies
				Removing ITC
				Removing KD (Teacher)
			Strengthening Unimodal Features
				Use unimodal encoders distilled in "Unimodal Knowledge Distillation"
				Masked Data Modeling as additional Loss for modality specific encoders(might make other task difficlut though, with IBN for MIM)
			Self-Distillation (Dino Loss)
			Distillation-less approach (Teacher stays for IBN of MIM)
			Multimodal Teacher
				
			Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Importance of the right Teacher (why BEiT-2? etc...) (D2V2 Image, BEiT-3, Dino, )
			Non-Knowledge-Distaillation (motivation -> application of kd very limited, especially with unimodal model)
				Remove ITC? and do finetuning?
			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
			Concatenated representations (Motivation: Seen in retrieval visualization that e.g. retrieved text is often very similar with correct one, but
			differs in small differences -> can only be solved by interaction between individual tokens)

		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
			Masked Vision-Language Modeling
			Additional Contribution: DistilBEiT-3

		Ablation Study: Reduction of Data (Mulitple reductions and plot retrieval and zero shot)
		
		
Conclusion
Outlook (Same vs. different pos encoding ^*, can architectures be mixed?, add other modalities, especially audio + add audio)
Lessons Learned
Appendix (plots, money spend etc.)
	Hyperparameters
	Pseudocode
	Figures and Visualizations
	Technical Details


^*
Architectural Studies
	Mixed Positional Encodings (defaults with pretrained D2V modules, include Token-Type Embeddings!)
	Shared Positional Encodings