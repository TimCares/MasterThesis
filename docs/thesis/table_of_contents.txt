EMKUM: Extracting Multimodal Knowledge from Unimodal Models
BYOML: Bootstrap your own multimodal latent

Introduction
Background
	Self-Supervised Learning
	Multimodal Models
	(Masked Modality (Image/Text) Modeling)
	Contrastive Learning and Retrieval
	Knowledge-Distillation
	Related Work
		Data2Vec
		Deep Aligned Representations
		CLIP
		FLAVA
		VLMo
		BEiT
Methodology
	Tools
    Experimental Approach
	Data and Preparation (mention uncurated nature -> imporant for scaling)
Experiments
	Notations and Definitions (... of image patches etc. -> if this changes for specific experiments, mention then and redefine)
	Unimodal Knowledge Distillation
	Multimodal Knowledge Distillation
		Reproducing: â€œSee, Hear, Read: Deep Aligned Representations"
		Aligned Representations
			Supervised Teacher
				Larger Batch Sizes with DDP
				Multimodal Transformer Block
			Self-Supervised Teacher
			Contrastive Learning
				On Zero-Shot Retrieval
				Short Captions (Region Descriptions)
				Projections for Task-Seperation
				Image-Text Matching with Feature-Fusion
				Increasing Negative Examples for ITC
				Ablation Study: Removing ITC
				
			Token-Type Embedding
			(Retaining Pretrained Knowledge)
			Target Cross-Modal Late Interaction
				Cross-Modal Late Interaction
				Method
				Empty Target
			Object-Level Regression
				Contrastive Target Loss (each teacher target is a class -> not 1000 classes, but 256 or 512, each being the cls token of one image passed to the teacher)
				Dimensionality Reduction (lin projection on both, teacher proj in ema of student proj)
				Feature Clusters
			Knowledge-Distillation Specific
				Dino Loss
				
				(Masked Vision-Language Modeling (same everything, just tokens randomly removed -> we can reuse teacher activations))
				(Biased VL Initialization (init from text))
				Fair Comparison with Supervised Teacher (bigger supervised teacher -> approx. 85 mill params)
			Importance of the right Teacher (why BEiT-2? etc...) (D2V2 Image, BEiT-3, Dino, )
			Non-Knowledge-Distaillation (motivation -> application of kd very limited, especially with unimodal model)
				Remove ITC? and do finetuning?
			Limitations (from trained models alone: No NRLV2, etc possible) -> mention https://arxiv.org/pdf/2102.03334 section 2.1 beginning page 3
		
		Aligned Referencing Representations
			Relevance (regarding: unimodal model as teacher no possible)
			Pretrain-CoCa (image encoder is beit2, maybe tune only upper two layers, unimodal text decoder is 6 layer d2v2 text, multimodal text decoder is 2 from scratch)
			Masked Vision-Language Modeling
			Additional Contribution: DistilBEiT-3

		Ablation Study: Reduction of Data (Mulitple reductions and plot retrieval and zero shot)
		
		
Conclusion
Outlook (Same vs. different pos encoding ^*)
Lessons Learned
Appendix (plots, money spend etc.)
	Hyperparameters
	Pseudocode
	Additional Visualizations
	Technical Details


^*
Architectural Studies
	Mixed Positional Encodings (defaults with pretrained D2V modules, include Token-Type Embeddings!)
	Shared Positional Encodings